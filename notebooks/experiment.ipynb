{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa7de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "MODELS_NAMES = ['all-MiniLM-L6-v2', \"all-mpnet-base-v2\"]\n",
    "DATA_PATH = '../data/products.csv'\n",
    "TEST_DATA_PATH = '../data/test_queries.csv'\n",
    "ARTIFACTS_DIR = '../app/artifacts'\n",
    "\n",
    "# Ensure artifacts directory exists\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8fd62",
   "metadata": {},
   "source": [
    "We load the data, both the official catalogue of products and the simulated users' queries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cde094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 products.\n",
      "Loaded 500 test queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>Makita Impact Driver 28Pro</td>\n",
       "      <td>Power Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Fluke Impact Driver 86X</td>\n",
       "      <td>Power Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>3M Utility Knife 68Pro</td>\n",
       "      <td>Hand Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>Milwaukee Tape Measure 16Pro</td>\n",
       "      <td>Hand Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>Knipex Face Shield 44Max</td>\n",
       "      <td>Safety Gear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                          name     category\n",
       "0  1000    Makita Impact Driver 28Pro  Power Tools\n",
       "1  1001       Fluke Impact Driver 86X  Power Tools\n",
       "2  1002        3M Utility Knife 68Pro   Hand Tools\n",
       "3  1003  Milwaukee Tape Measure 16Pro   Hand Tools\n",
       "4  1004      Knipex Face Shield 44Max  Safety Gear"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(df_products)} products.\")\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"Loaded {len(df_test)} test queries.\")\n",
    "\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7479361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close any stuck runs\n",
    "if mlflow.active_run():\n",
    "    print(\"Ending existing run...\")\n",
    "    mlflow.end_run()\n",
    "    \n",
    "tracking_uri = \"file:../mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"Project2_Semantic_Search\")\n",
    "results_table= []\n",
    "k_eval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac20638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Index...\n",
      "Running Inference...\n",
      "Testing Model: all-mpnet-base-v2...\n",
      "Building Index...\n",
      "Running Inference...\n",
      "Table of results\n",
      "               Model  Recall@1  Recall@5       MRR  Latency (s)  \\\n",
      "0   all-MiniLM-L6-v2     0.336     0.822  0.516333     0.001779   \n",
      "1  all-mpnet-base-v2     0.306     0.780  0.485067     0.010243   \n",
      "\n",
      "   Conf (Correct)  Conf (Wrong)  \n",
      "0        0.724082      0.652440  \n",
      "1        0.693491      0.634612  \n"
     ]
    }
   ],
   "source": [
    "for model_name in MODELS_NAMES:\n",
    "    # unique run name so they don't get mixed up\n",
    "    with mlflow.start_run(run_name=f\"Run_{model_name}\"):\n",
    "        print(f\"Testing Model: {model_name}...\")\n",
    "        \n",
    "        # Load Model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"k\", k_eval)\n",
    "        \n",
    "        # Embed Products & Build Index\n",
    "        print(\"Building Index...\")\n",
    "        product_embeddings = model.encode(df_products['name'].tolist())\n",
    "        faiss.normalize_L2(product_embeddings)\n",
    "        \n",
    "        d = product_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(d)\n",
    "        index.add(product_embeddings)\n",
    "        \n",
    "        # Save artifacts locally so we can log them\n",
    "        faiss.write_index(index, os.path.join(ARTIFACTS_DIR, \"faiss_index.bin\"))\n",
    "        df_products.to_pickle(os.path.join(ARTIFACTS_DIR, \"products.pkl\"))\n",
    "\n",
    "        # Evaluate\n",
    "        print(\"Running Inference...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        query_embeddings = model.encode(df_test['query'].tolist())\n",
    "        faiss.normalize_L2(query_embeddings)\n",
    "        D, I = index.search(query_embeddings, k_eval)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        avg_latency = inference_time / len(df_test)\n",
    "        \n",
    "        # Calculate Detailed Metrics\n",
    "        recall_1 = 0\n",
    "        recall_5 = 0\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        # Calibration: Correct vs Incorrect scores\n",
    "        correct_scores = []\n",
    "        incorrect_scores = []\n",
    "        \n",
    "        for i, row in df_test.iterrows():\n",
    "            expected_id = row['expected_id']\n",
    "            result_indices = I[i]\n",
    "            result_scores = D[i]\n",
    "            \n",
    "            found_rank = None\n",
    "            \n",
    "            # Check matches\n",
    "            for rank, (idx, score) in enumerate(zip(result_indices, result_scores)):\n",
    "                predicted_product = df_products.iloc[idx]\n",
    "                \n",
    "                # Calibration Data Collection\n",
    "                if predicted_product['id'] == expected_id:\n",
    "                    correct_scores.append(score)\n",
    "                    if found_rank is None: # First time finding it\n",
    "                        found_rank = rank + 1\n",
    "                else:\n",
    "                    incorrect_scores.append(score)\n",
    "\n",
    "            # Calculate Recall & MRR\n",
    "            if found_rank:\n",
    "                reciprocal_ranks.append(1 / found_rank)\n",
    "                if found_rank == 1:\n",
    "                    recall_1 += 1\n",
    "                if found_rank <= 5:\n",
    "                    recall_5 += 1\n",
    "            else:\n",
    "                reciprocal_ranks.append(0.0)\n",
    "        \n",
    "        # Averages\n",
    "        score_r1 = recall_1 / len(df_test)\n",
    "        score_r5 = recall_5 / len(df_test)\n",
    "        score_mrr = sum(reciprocal_ranks) / len(df_test)\n",
    "        \n",
    "        # Calibration Metric\n",
    "        avg_conf_correct = np.mean(correct_scores) if correct_scores else 0\n",
    "        avg_conf_incorrect = np.mean(incorrect_scores) if incorrect_scores else 0\n",
    "        \n",
    "        \n",
    "        mlflow.log_metric(\"recall_at_1\", score_r1)\n",
    "        mlflow.log_metric(\"recall_at_5\", score_r5)\n",
    "        mlflow.log_metric(\"mrr\", score_mrr)\n",
    "        mlflow.log_metric(\"avg_latency\", avg_latency)\n",
    "        mlflow.log_metric(\"calibration_correct_score\", avg_conf_correct)\n",
    "        mlflow.log_metric(\"calibration_incorrect_score\", avg_conf_incorrect)\n",
    "        \n",
    "        # Log Artifacts\n",
    "        mlflow.log_artifact(os.path.join(ARTIFACTS_DIR, \"faiss_index.bin\"))\n",
    "        mlflow.log_artifact(os.path.join(ARTIFACTS_DIR, \"products.pkl\"))\n",
    "        \n",
    "        results_table.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Recall@1\": score_r1,\n",
    "            \"Recall@5\": score_r5,\n",
    "            \"MRR\": score_mrr,\n",
    "            \"Latency (s)\": avg_latency,\n",
    "            \"Conf (Correct)\": avg_conf_correct,\n",
    "            \"Conf (Wrong)\": avg_conf_incorrect\n",
    "        })\n",
    "\n",
    "\n",
    "print(\"Table of results\")\n",
    "df_results = pd.DataFrame(results_table)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2f42f",
   "metadata": {},
   "source": [
    "save the production model artifacsts, we chose all-MiniLM-L-v2 as it is lighter and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a60452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "embeddings = model.encode(df_products['name'].tolist())\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "d = embeddings.shape[1]  \n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, f\"{ARTIFACTS_DIR}/faiss_index.bin\")\n",
    "df_products.to_pickle(f\"{ARTIFACTS_DIR}/products.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
