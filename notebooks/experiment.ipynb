{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa7de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "MODELS_NAMES = ['all-MiniLM-L6-v2', \"all-mpnet-base-v2\"]\n",
    "DATA_PATH = '../data/products.csv'\n",
    "TEST_DATA_PATH = '../data/test_queries.csv'\n",
    "ARTIFACTS_DIR = '../app/artifacts'\n",
    "\n",
    "# Ensure artifacts directory exists\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8fd62",
   "metadata": {},
   "source": [
    "We load the data, both the official catalogue of products and the simulated users' queries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cde094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 products.\n",
      "Loaded 5 test queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Bosch Professional Drill GSB 18V</td>\n",
       "      <td>Power Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Makita Cordless Impact Driver</td>\n",
       "      <td>Power Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Industrial Safety Helmet (Yellow)</td>\n",
       "      <td>Safety Gear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>3M Protective Safety Goggles</td>\n",
       "      <td>Safety Gear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Fluke Digital Multimeter 117</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                               name     category\n",
       "0  101   Bosch Professional Drill GSB 18V  Power Tools\n",
       "1  102      Makita Cordless Impact Driver  Power Tools\n",
       "2  103  Industrial Safety Helmet (Yellow)  Safety Gear\n",
       "3  104       3M Protective Safety Goggles  Safety Gear\n",
       "4  105       Fluke Digital Multimeter 117  Electronics"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(df_products)} products.\")\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"Loaded {len(df_test)} test queries.\")\n",
    "\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7479361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/nlp-semantic-match/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "# Close any stuck runs\n",
    "if mlflow.active_run():\n",
    "    print(\"Ending existing run...\")\n",
    "    mlflow.end_run()\n",
    "    \n",
    "tracking_uri = \"file:../mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"Project2_Semantic_Search\")\n",
    "results_table= []\n",
    "k_eval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac20638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Building Index...\n",
      "   -> Running Inference...\n",
      "   -> Recall@1             | 1.00\n",
      "   -> Recall@5             | 1.00\n",
      "   -> MRR                  | 1.0000\n",
      "   -> Latency              | 0.0043s\n",
      "   -> Avg Score(Correct)   | 0.6674\n",
      "   -> Avg Score(Wrong)     | 0.2363\n",
      "--------------------------------------------------\n",
      "Testing Model: all-mpnet-base-v2...\n",
      "   -> Building Index...\n",
      "   -> Running Inference...\n",
      "   -> Recall@1             | 1.00\n",
      "   -> Recall@5             | 1.00\n",
      "   -> MRR                  | 1.0000\n",
      "   -> Latency              | 0.0185s\n",
      "   -> Avg Score(Correct)   | 0.6858\n",
      "   -> Avg Score(Wrong)     | 0.2824\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ FINAL COMPARISON RESULTS üèÜ\n",
      "               Model  Recall@1  Recall@5  MRR  Latency (s)  Conf (Correct)  \\\n",
      "0   all-MiniLM-L6-v2       1.0       1.0  1.0     0.004348        0.667361   \n",
      "1  all-mpnet-base-v2       1.0       1.0  1.0     0.018530        0.685788   \n",
      "\n",
      "   Conf (Wrong)  \n",
      "0      0.236273  \n",
      "1      0.282373  \n",
      "\n",
      "üí° INTERPRETATION:\n",
      "The smaller model (MiniLM) is preferred: Similar accuracy but much faster.\n"
     ]
    }
   ],
   "source": [
    "for model_name in MODELS_NAMES:\n",
    "    # unique run name so they don't get mixed up\n",
    "    with mlflow.start_run(run_name=f\"Run_{model_name}\"):\n",
    "        print(f\"Testing Model: {model_name}...\")\n",
    "        \n",
    "        # Load Model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"k\", k_eval)\n",
    "        \n",
    "        # Embed Products & Build Index\n",
    "        print(\"Building Index...\")\n",
    "        product_embeddings = model.encode(df_products['name'].tolist())\n",
    "        faiss.normalize_L2(product_embeddings)\n",
    "        \n",
    "        d = product_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(d)\n",
    "        index.add(product_embeddings)\n",
    "        \n",
    "        # Save artifacts locally so we can log them\n",
    "        faiss.write_index(index, os.path.join(ARTIFACTS_DIR, \"faiss_index.bin\"))\n",
    "        df_products.to_pickle(os.path.join(ARTIFACTS_DIR, \"products.pkl\"))\n",
    "\n",
    "        # Evaluate\n",
    "        print(\"Running Inference...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        query_embeddings = model.encode(df_test['query'].tolist())\n",
    "        faiss.normalize_L2(query_embeddings)\n",
    "        D, I = index.search(query_embeddings, k_eval)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        avg_latency = inference_time / len(df_test)\n",
    "        \n",
    "        # Calculate Detailed Metrics\n",
    "        recall_1 = 0\n",
    "        recall_5 = 0\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        # Calibration: Correct vs Incorrect scores\n",
    "        correct_scores = []\n",
    "        incorrect_scores = []\n",
    "        \n",
    "        for i, row in df_test.iterrows():\n",
    "            expected_id = row['expected_id']\n",
    "            result_indices = I[i]\n",
    "            result_scores = D[i]\n",
    "            \n",
    "            found_rank = None\n",
    "            \n",
    "            # Check matches\n",
    "            for rank, (idx, score) in enumerate(zip(result_indices, result_scores)):\n",
    "                predicted_product = df_products.iloc[idx]\n",
    "                \n",
    "                # Calibration Data Collection\n",
    "                if predicted_product['id'] == expected_id:\n",
    "                    correct_scores.append(score)\n",
    "                    if found_rank is None: # First time finding it\n",
    "                        found_rank = rank + 1\n",
    "                else:\n",
    "                    incorrect_scores.append(score)\n",
    "\n",
    "            # Calculate Recall & MRR\n",
    "            if found_rank:\n",
    "                reciprocal_ranks.append(1 / found_rank)\n",
    "                if found_rank == 1:\n",
    "                    recall_1 += 1\n",
    "                if found_rank <= 5:\n",
    "                    recall_5 += 1\n",
    "            else:\n",
    "                reciprocal_ranks.append(0.0)\n",
    "        \n",
    "        # Averages\n",
    "        score_r1 = recall_1 / len(df_test)\n",
    "        score_r5 = recall_5 / len(df_test)\n",
    "        score_mrr = sum(reciprocal_ranks) / len(df_test)\n",
    "        \n",
    "        # Calibration Metric\n",
    "        avg_conf_correct = np.mean(correct_scores) if correct_scores else 0\n",
    "        avg_conf_incorrect = np.mean(incorrect_scores) if incorrect_scores else 0\n",
    "        \n",
    "        \n",
    "        mlflow.log_metric(\"recall_at_1\", score_r1)\n",
    "        mlflow.log_metric(\"recall_at_5\", score_r5)\n",
    "        mlflow.log_metric(\"mrr\", score_mrr)\n",
    "        mlflow.log_metric(\"avg_latency\", avg_latency)\n",
    "        mlflow.log_metric(\"calibration_correct_score\", avg_conf_correct)\n",
    "        mlflow.log_metric(\"calibration_incorrect_score\", avg_conf_incorrect)\n",
    "        \n",
    "        # Log Artifacts\n",
    "        mlflow.log_artifact(os.path.join(ARTIFACTS_DIR, \"faiss_index.bin\"))\n",
    "        mlflow.log_artifact(os.path.join(ARTIFACTS_DIR, \"products.pkl\"))\n",
    "        \n",
    "        results_table.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Recall@1\": score_r1,\n",
    "            \"Recall@5\": score_r5,\n",
    "            \"MRR\": score_mrr,\n",
    "            \"Latency (s)\": avg_latency,\n",
    "            \"Conf (Correct)\": avg_conf_correct,\n",
    "            \"Conf (Wrong)\": avg_conf_incorrect\n",
    "        })\n",
    "\n",
    "\n",
    "print(\"Table of results\")\n",
    "df_results = pd.DataFrame(results_table)\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
