{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa7de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "DATA_PATH = '../data/products.csv'\n",
    "TEST_DATA_PATH = '../data/test_queries.csv'\n",
    "ARTIFACTS_DIR = '../app/artifacts'\n",
    "\n",
    "# Ensure artifacts directory exists\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8fd62",
   "metadata": {},
   "source": [
    "We load the data, both the official catalogue of products and the simulated users' queries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cde094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 products.\n",
      "Loaded 5 test queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Bosch Professional Drill GSB 18V</td>\n",
       "      <td>Power Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Makita Cordless Impact Driver</td>\n",
       "      <td>Power Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Industrial Safety Helmet (Yellow)</td>\n",
       "      <td>Safety Gear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>3M Protective Safety Goggles</td>\n",
       "      <td>Safety Gear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Fluke Digital Multimeter 117</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                               name     category\n",
       "0  101   Bosch Professional Drill GSB 18V  Power Tools\n",
       "1  102      Makita Cordless Impact Driver  Power Tools\n",
       "2  103  Industrial Safety Helmet (Yellow)  Safety Gear\n",
       "3  104       3M Protective Safety Goggles  Safety Gear\n",
       "4  105       Fluke Digital Multimeter 117  Electronics"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(df_products)} products.\")\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"Loaded {len(df_test)} test queries.\")\n",
    "\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09590bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for products...\n",
      "Embeddings shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(\"Generating embeddings for products...\")\n",
    "# We encode the 'name' column. \n",
    "# We convert to numpy array because FAISS needs it.\n",
    "product_embeddings = model.encode(df_products['name'].tolist())\n",
    "\n",
    "# Normalize vectors (Important for Cosine Similarity in FAISS)\n",
    "faiss.normalize_L2(product_embeddings)\n",
    "\n",
    "print(f\"Embeddings shape: {product_embeddings.shape}\") \n",
    "# (10, 384) -> 10 products, 384 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02ca6a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built. Total vectors: 10\n"
     ]
    }
   ],
   "source": [
    "dimension = product_embeddings.shape[1] # 384 for MiniLM\n",
    "\n",
    "# Create Index\n",
    "index = faiss.IndexFlatIP(dimension) \n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(product_embeddings)\n",
    "\n",
    "print(f\"Index built. Total vectors: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "185b0597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to ../app/artifacts\n"
     ]
    }
   ],
   "source": [
    "# Save the FAISS index\n",
    "faiss.write_index(index, f\"{ARTIFACTS_DIR}/faiss_index.bin\")\n",
    "\n",
    "# Save the product dataframe (to map ID back to Name)\n",
    "df_products.to_pickle(f\"{ARTIFACTS_DIR}/products.pkl\")\n",
    "\n",
    "print(f\"Artifacts saved to {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac20638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Evaluation Run...\n",
      "Metric                    | Value     \n",
      "----------------------------------------\n",
      "Recall@1                  | 1.00\n",
      "Recall@5                  | 1.00\n",
      "MRR                       | 1.0000\n",
      "Avg Latency (sec)         | 0.0031\n",
      "Avg Score (Correct)       | 0.6674\n",
      "Avg Score (Wrong)         | 0.2363\n",
      "----------------------------------------\n",
      "✅ All metrics logged successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- SAFETY CHECK: Close any stuck runs first ---\n",
    "if mlflow.active_run():\n",
    "    print(\"Ending existing run...\")\n",
    "    mlflow.end_run()\n",
    "    \n",
    "# --- CONFIGURATION ---\n",
    "tracking_uri = \"file:../mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"Project2_Semantic_Search\")\n",
    "\n",
    "print(\"Starting Final Evaluation Run...\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # 1. Setup Parameters\n",
    "    k_eval = 5  # We need top 5 for Recall@5\n",
    "    mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "    mlflow.log_param(\"k\", k_eval)\n",
    "\n",
    "    # 2. Run Inference & Measure Latency\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_embeddings = model.encode(df_test['query'].tolist())\n",
    "    faiss.normalize_L2(query_embeddings)\n",
    "    D, I = index.search(query_embeddings, k_eval)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    avg_latency = inference_time / len(df_test)\n",
    "    \n",
    "    # 3. Calculate Metrics\n",
    "    recall_1 = 0\n",
    "    recall_5 = 0\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    # For Calibration: We separate scores of correct vs incorrect predictions\n",
    "    correct_scores = []\n",
    "    incorrect_scores = []\n",
    "\n",
    "    for i, row in df_test.iterrows():\n",
    "        expected_id = row['expected_id']\n",
    "        result_indices = I[i]\n",
    "        result_scores = D[i]\n",
    "        \n",
    "        found_rank = None\n",
    "        \n",
    "        # Check matches\n",
    "        for rank, (idx, score) in enumerate(zip(result_indices, result_scores)):\n",
    "            predicted_product = df_products.iloc[idx]\n",
    "            \n",
    "            # Calibration Data Collection\n",
    "            # We treat cosine similarity as \"confidence\"\n",
    "            if predicted_product['id'] == expected_id:\n",
    "                correct_scores.append(score)\n",
    "                if found_rank is None: # First time finding it\n",
    "                    found_rank = rank + 1\n",
    "            else:\n",
    "                incorrect_scores.append(score)\n",
    "\n",
    "        # Calculate Recall & MRR\n",
    "        if found_rank:\n",
    "            reciprocal_ranks.append(1 / found_rank)\n",
    "            if found_rank == 1:\n",
    "                recall_1 += 1\n",
    "            if found_rank <= 5:\n",
    "                recall_5 += 1\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    # 4. Final Math\n",
    "    score_r1 = recall_1 / len(df_test)\n",
    "    score_r5 = recall_5 / len(df_test)\n",
    "    score_mrr = sum(reciprocal_ranks) / len(df_test)\n",
    "    \n",
    "    # Calibration Metric: \n",
    "    # A simple way to measure this: Avg Score when Correct vs Avg Score when Wrong.\n",
    "    # If the model is \"Calibrated\", Correct Scores should be significantly higher.\n",
    "    avg_conf_correct = np.mean(correct_scores) if correct_scores else 0\n",
    "    avg_conf_incorrect = np.mean(incorrect_scores) if incorrect_scores else 0\n",
    "    calibration_gap = avg_conf_correct - avg_conf_incorrect\n",
    "\n",
    "    # 5. Log & Print\n",
    "    print(f\"{'Metric':<25} | {'Value':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Recall@1':<25} | {score_r1:.2f}\")\n",
    "    print(f\"{'Recall@5':<25} | {score_r5:.2f}\")\n",
    "    print(f\"{'MRR':<25} | {score_mrr:.4f}\")\n",
    "    print(f\"{'Avg Latency (sec)':<25} | {avg_latency:.4f}\")\n",
    "    print(f\"{'Avg Score (Correct)':<25} | {avg_conf_correct:.4f}\")\n",
    "    print(f\"{'Avg Score (Wrong)':<25} | {avg_conf_incorrect:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    mlflow.log_metric(\"recall_at_1\", score_r1)\n",
    "    mlflow.log_metric(\"recall_at_5\", score_r5)\n",
    "    mlflow.log_metric(\"mrr\", score_mrr)\n",
    "    mlflow.log_metric(\"avg_latency\", avg_latency)\n",
    "    mlflow.log_metric(\"calibration_correct_score\", avg_conf_correct)\n",
    "    \n",
    "    # Save Artifacts\n",
    "    mlflow.log_artifact(f\"{ARTIFACTS_DIR}/faiss_index.bin\")\n",
    "    mlflow.log_artifact(f\"{ARTIFACTS_DIR}/products.pkl\")\n",
    "    \n",
    "    print(\"✅ All metrics logged successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
